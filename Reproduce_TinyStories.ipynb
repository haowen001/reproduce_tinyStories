{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b834f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets==2.18.0    # 测试时发现 2.19.0 有点小问题，稳妥起见用 2.18.0\n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ee53d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'                       # 禁用 wandb，也可以不用这一条\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'     # 设置 device，能用 cuda 就用 cuda，苹果 M 系列可以用 mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2365146",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d20fc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "intermediate_size = (int(hidden_size * 8/3 / 128) + 1) * 128\n",
    "\n",
    "config = AutoConfig.for_model(\n",
    "    model_type=\"llama\",\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=4,\n",
    "    num_key_value_heads=8\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a441e28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(\n",
    "    config,\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc6be9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 打印模型的每一层及其参数大小\n",
    "def print_model_parameters(model):\n",
    "    print(\"Layer Name & Parameters\")\n",
    "    print(\"----------------------------\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param_size = parameter.size()\n",
    "        param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "        total_params += param_count\n",
    "        print(f\"{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}\")\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Total Parameters: {total_params} ({total_params / 1000000:.1f} M)\")\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29dfa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    input_text: str = \"Once upon a time, \",\n",
    "    max_new_tokens: int = 16\n",
    "):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(generated_text)\n",
    "\n",
    "inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fb1d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Kaiming 初始化\n",
    "def kaiming_initialization(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.dim() > 1:\n",
    "            torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        elif 'bias' in name:\n",
    "            # 一般偏置项可以初始化为0\n",
    "            torch.nn.init.constant_(param, 0)\n",
    "\n",
    "kaiming_initialization(model)\n",
    "inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faff0ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 应用全部训练集，约 2.7 M\n",
    "# ds_train = load_dataset('noanabeshima/TinyStoriesV2', split='train')\n",
    "# 这里可以调整比例，我只用了 10%，约 270 K\n",
    "ds_train = load_dataset('noanabeshima/TinyStoriesV2', split='train[:10%]')\n",
    "ds_val = load_dataset('noanabeshima/TinyStoriesV2', split='validation')\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0d9b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 查看一下数据示例\n",
    "ds_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143f27d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def process_func(examples: Dict[str, List]):\n",
    "    max_token = 2048\n",
    "\n",
    "    encoded_texts = tokenizer(examples['text'], add_special_tokens=False)\n",
    "    input_ids_list = encoded_texts['input_ids']\n",
    "\n",
    "    new_input_ids_list, new_attn_mask_list = [], []\n",
    "    for input_ids in input_ids_list:\n",
    "        temp = input_ids[-max_token+1:] + [tokenizer.eos_token_id]\n",
    "        new_input_ids_list.append(temp)\n",
    "        new_attn_mask_list.append([1] * len(temp))\n",
    "    return {\n",
    "        \"input_ids\": new_input_ids_list,\n",
    "        \"attention_mask\": new_attn_mask_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6481e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.shuffle()\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    process_func,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=ds_train.column_names,\n",
    "    desc='Running tokenizer on train_set: '\n",
    ")\n",
    "ds_val = ds_val.map(\n",
    "    process_func,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=ds_val.column_names,\n",
    "    desc='Running tokenizer on val_set: '\n",
    ")\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ebd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99fe04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='saves',\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=50,\n",
    "    report_to=None,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    seed=3407\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d337634",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07c928",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 启动训练\n",
    "# 这里只 train 了 2 epochs，loss 收敛到了 1.6 左右\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d65140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"Once upon a time, in a beautiful garden, there lived a little rabbit named Peter Rabbit.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
